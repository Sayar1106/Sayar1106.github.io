[{"authors":null,"categories":null,"content":"Sayar Banerjee is a Analyst at a start up company called Indihood based out of Bangalore, India. His research interests include Machine Learning, Artificial Intelligence and Blockchain.\nBeing Indihood\u0026rsquo;s sole data science professional, he is responsible for creating and maintaining big data processing architecture, constructing data warehouses, building data science and machine learning capabilities, and improving automation for company operations.\n","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Sayar Banerjee is a Analyst at a start up company called Indihood based out of Bangalore, India. His research interests include Machine Learning, Artificial Intelligence and Blockchain.\nBeing Indihood\u0026rsquo;s sole data science professional, he is responsible for creating and maintaining big data processing architecture, constructing data warehouses, building data science and machine learning capabilities, and improving automation for company operations.","tags":null,"title":"Sayar Banerjee","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://Sayar1106.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":[],"categories":[],"content":"","date":1638257180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638257180,"objectID":"c7de08b0124470734c1ca16d2fd6f39e","permalink":"https://Sayar1106.github.io/project/hotdogornot/","publishdate":"2021-11-30T12:56:20+05:30","relpermalink":"/project/hotdogornot/","section":"project","summary":"Inspired by the T.V show Silicon Valley, this API predicts whether an image is that of a hot dog or not.","tags":["Deep Learning","API"],"title":"Hot Dog or Not","type":"project"},{"authors":[],"categories":[],"content":"","date":1638257171,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638257171,"objectID":"7e7db3481611b48f2c16c0d4fd59ad3e","permalink":"https://Sayar1106.github.io/project/heart-disease/","publishdate":"2021-11-30T12:56:11+05:30","relpermalink":"/project/heart-disease/","section":"project","summary":"A Machine Learning Application that predicts whether a customer has heart disease based on 14 different parameters.","tags":["Web App","Machine Learning"],"title":"Heart Disease","type":"project"},{"authors":[],"categories":[],"content":"","date":1638257162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638257162,"objectID":"9cccaa4ac77a120d684469a88d8cce8f","permalink":"https://Sayar1106.github.io/project/covid-dashboard/","publishdate":"2021-11-30T12:56:02+05:30","relpermalink":"/project/covid-dashboard/","section":"project","summary":"A data visualization web application created using Python that updates covid data daily.","tags":["Web App"],"title":"Covid Dashboard","type":"project"},{"authors":[],"categories":[],"content":"","date":1638256911,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638256911,"objectID":"da51607d3d7e84d77caa0b51705d81c3","permalink":"https://Sayar1106.github.io/project/awesome-data-scientist/","publishdate":"2021-11-30T12:51:51+05:30","relpermalink":"/project/awesome-data-scientist/","section":"project","summary":"A roadmap to become a Data Scientist in 2021","tags":["Other"],"title":"Awesome Data Scientist","type":"project"},{"authors":[],"categories":[],"content":"Hi all, this article will explore the process of deploying a FastAPI application on Saturn Cloud. FastAPI is a robust web framework for building APIs with the Python language. Saturn Cloud is a platform dedicated to scaling Machine Learning and Big Data pipelines and more.\nThe model will predict median house prices in California. Let\u0026rsquo;s jump right into it.\nResources 👉 Repository\n👉 FastAPI\n👉 Scikit-learn\n👉 Joblib\n Data Exploration The dataset I will use for training our machine learning model is called \u0026ldquo;California Housing Prices.\u0026rdquo; It can be found here.\nThe contents of our data are as follows:\nThe data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned, the data isn\u0026rsquo;t clean, so there are some preprocessing steps required! The columns are as follows, and their names are pretty self-explanatory:\n longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_ value ocean_proximity  On doing a rudimentary exploration of the dataset, I found the following:\n   A correlation plot between all the numerical features shows us the following:\n   According to the graph, most numerical features have very little correlation with median_house_value except median_income, which seems to have a strong positive correlation of around 0.68.\n Data Cleaning/Feature Engineering Since the total_bedrooms feature had missing values, I had to impute it. For simplicity, I chose the median as the metric to impute the feature.\nAdditionally, two new features were engineered, namely, \u0026ldquo;rooms_per_households\u0026rdquo; and \u0026ldquo;population_per_household.\u0026rdquo;\n    Training the Model Our repository looks like this:\n   The requirements.txt file contains our dependencies. It is crucial to have all the dependencies added to this file as it will be used during our deployment.\n   The file src/main.py contains our training script. Let us take a look at some of the essential functions in the script.\nOur training model pipeline is relatively standard. There is just one categorical column (ocean_proximity). For the other numerical columns, I applied a standard scaler. The ColumnTransformer estimator helps to facilitate feature transformations on heterogeneous data.\nAs for the model, I chose the Random Forest algorithm. I created the pipeline using scikit-learn\u0026rsquo;s Pipeline class.\n   I used joblib to save our model. Since the model file was quite large (\u0026gt;100Mb), I decided to store it in AWS S3. The model\u0026rsquo;s R² score was around 0.81, and the RMSE was around 49k.\n Setting up FastAPI Server and Frontend As you may have guessed, app/main.py contains our code for the server. Since the model is stored in AWS, I used boto3 to download a local copy to the server.\nIf your bucket and file are private, you may need to set up authentication to access it on Saturn Cloud. You can do it by following this guide.\nI wrote a simple function to load our model from AWS:\n   The variables BUCKET_NAME and FILE_NAME are self-explanatory. LOCAL_PATH is the path to where the model will be copied locally.\nI also defined global variables for the app, model, and templates.\n   Homepage Since I\u0026rsquo;m creating an application, it\u0026rsquo;s essential to have a homepage to serve as an interface for the model server.\nI created a homepage for the app so that users can enter values for each of the features. To render the page, I used Jinja2Templates, which is provided out of the box by FastAPI templates.TemplateResponse renders our landing page titled \u0026ldquo;index.html.\u0026rdquo;\n   index.html contains a form that will serve as the frontend for our application. The body of the page looks like this:\n   If you look closely at the form tag, you will see that the action attribute is set to \u0026ldquo;/submitform\u0026rdquo; and the request method is a POST request.\n   Our FastAPI server needs to have a method that handles the form data. This method needs to be decorated by app.post(\u0026quot;/submitform\u0026quot;) to handle the request appropriately.\n   You will notice that each of the variables is set as form parameters using Form. This class tells FastAPI that each variable\u0026rsquo;s input is being received from a form.\nYou will also notice that line 26 has a method called predict. This method is actually where the model pipeline is fed the input from the form using the appropriate format. Since the pipeline can only receive input from a data frame, I first convert the data into a data frame. I then created the features as part of the feature engineering process. Finally, I return the model\u0026rsquo;s predictions.\n   Once I had the price prediction, I used templates.TemplateResponse again to return a page called result.html. Along with \u0026ldquo;request\u0026rdquo;, I also passed \u0026ldquo;price\u0026rdquo; through the TemplateResponse method. Finally, I rendered the price on the body of result.html.\n    Deploying to Saturn Cloud Before setting up the deployment, I pushed all of the code to Github. To deploy it, you must have your repository connected to Saturn Cloud. To do so, you can follow this guide.\nOnce your repo is connected, head over to resources and select \u0026ldquo;New Deployment\u0026rdquo;.\n   After this, you will be greeted with a form:\n   There are a few things to note when filling out the form. For instance, the \u0026ldquo;Command\u0026rdquo; is what the deployment will run to start your application.\n   Note that Saturn Cloud requires your applications to listen using port 8000.\nAlso, note the Extra Packages header. This is the script that will be used to install additional packages before the command is run. Since Saturn Cloud\u0026rsquo;s default image does not have certain libraries like FastAPI and Uvicorn, pass \u0026ldquo;-r requirements.txt\u0026rdquo; to the text box.\nThis ensures that the script \u0026ldquo;`pip install -r requirements.txt` \u0026ldquo;is run before startup, containing dependencies for the additional packages.\nNote that you can also write the individual names of each package in this section to install them.\nOnce you hit the Create button, your deployment will be created. Click on it and add your Github repo to the deployment. Ensure that you add the path to the Github resource to your working directory. Once that is done, click the green arrow to start the deployment.\n   Once your deployment is ready, click on the public URL. You should see a page like this:\n   Once you fill out the form, you will see a page with the predicted price:\n   Note that I used the last example of my test set as input. The actual median house price was $133000, so the model did a reasonably good job! 😀\n👉 Link to the Github directory\n Conclusion Congratulations! You have successfully learned how to deploy a FastAPI model on Saturn Cloud! If you\u0026rsquo;re curious about using their environment, they offer 30 free hours a month for data scientists and teams. I hope you enjoyed reading this article. Until next time! ✋\n","date":1637280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637280000,"objectID":"36f9ab42ffef953437dbfe766c1672b4","permalink":"https://Sayar1106.github.io/post/fastapi-saturncloud/","publishdate":"2021-11-19T00:00:00Z","relpermalink":"/post/fastapi-saturncloud/","section":"post","summary":"Hi all, this article will explore the process of deploying a FastAPI application on Saturn Cloud. FastAPI is a robust web framework for building APIs with the Python language. Saturn Cloud is a platform dedicated to scaling Machine Learning and Big Data pipelines and more.","tags":[],"title":"Hosting FastAPI with Saturn Cloud Deployments","type":"post"},{"authors":[],"categories":[],"content":" “Finding patterns is easy in any kind of data-rich environment; that’s what mediocre gamblers do. The key is in determining whether the patterns represent noise or signal.”\n― Nate Silver\n This article is part 2 of my “Fast Feature Engineering” series. If you have not read my first article which talks about tabular data, then I request you to check it out here:\nFast Feature Engineering in Python: Tabular Data\nThis article will look at some of the best practices to follow when performing image processing as part of our machine learning workflow.\n Libraries import random from PIL import Image import cv2 import numpy as np from matplotlib import pyplot as plt import json import albumentations as A import torch import torchvision.models as models import torchvision.transforms as transforms import torch.nn as nn from tqdm import tqdm_notebook from torch.utils.data import DataLoader from torchvision.datasets import CIFAR10   Resize/Scale Images Resizing is the most fundamental transformation done by deep learning practitioners in the field. The primary reason for doing this is to ensure that the input received by our deep learning system is consistent.\nAnother reason for resizing is to reduce the number of parameters in the model. Smaller dimensions signify a smaller neural network and hence, saves us the time and computation power required to train our model.\nWhat about the loss of information? Some information is indeed lost when you resize down from a larger image. However, depending on your task, you can choose how much information you’re willing to sacrifice for training time and compute resources.\nFor example, an object detection task will require you to maintain the image\u0026rsquo;s aspect ratio since the goal is to detect the exact position of objects.\nIn contrast, an image classification task may require you to resize all images down to a specified size (224 x 224 is a good rule of thumb).\n   After resizing our image looks like this:\n   Why perform image scaling? Similar to tabular data, scaling images for classification tasks can help our deep learning model\u0026rsquo;s learning rate to converge to the minima better.\nScaling ensures that a particular dimension does not dominate others. I found a fantastic answer on StackExchange regarding this. You can read it here.\nOne type of feature scaling is the process of standardizing our pixel values. We do this by subtracting the mean of each channel from its pixel value and then divide it via standard deviation.\nThis is a popular choice of feature engineering when training models for classification tasks.\nmean = np.mean(img_resized, axis=(1,2), keepdims=True) std = np.std(img_resized, axis=(1,2), keepdims=True) img_std = (img_resized - mean) / std  Note: Like resizing, one may not want to do image scaling when performing object detection and image generation tasks.\nThe example code above demonstrates the process of scaling an image via standardization. There are other forms of scaling such as centering and normalization.\n Augmentations (Classification) The primary motivation behind augmenting images is due to the appreciable data requirement for computer vision tasks. Often, obtaining enough images for training can prove to be challenging for a multitude of reasons.\nImage augmentation enables us to create new training samples by slightly modifying the original ones.\nIn this example, we will look at how to apply vanilla augmentations for a classification task. We can use the out of the box implementations of the Albumentations library to do this:\n       By applying image augmentations, our deep learning models can generalize better to the task (avoid overfitting), thereby increasing its predictive power on unseen data.\n Augmentations (Object Detection) The Albumentations library can also be used to create augmentations for other tasks such as object detections. Object detection requires us to create bounding boxes around the object of interest.\nWorking with raw data can prove to be challenging when trying to annotate images with the coordinates for the bounding boxes.\nFortunately, there are many publicly and freely available datasets that we can use to create an augmentation pipeline for object detection. One such dataset is the Chess Dataset.\nThe dataset contains 606 images of chess pieces on a chessboard.\nAlong with the images, a JSON file is provided that contains all the information pertaining to the bounding boxes for each chess piece in a single image.\nBy writing a simple function, we can visualize the data after the augmentation is applied:\nwith open(\u0026quot;_annotations.coco.json\u0026quot;) as f: json_file = json.load(f) x_min, y_min, w, h = json_file['annotations'][0]['bbox'] x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h) def visualize_bbox(img, bbox, class_name, color=(0, 255, 0), thickness=2): x_min, y_min, w, h = bbox x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h) cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness) ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1) cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1) cv2.putText( img, text=class_name, org=(x_min, y_min - int(0.3 * text_height)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.35, color=(255, 255, 255), lineType=cv2.LINE_AA, ) return img bbox_img = visualize_bbox(np.array(img), json_file['annotations'][0]['bbox'], class_name=json_file['categories'][0]['name']) Image.fromarray(bbox_img)       Now, let’s try to create an augmentation pipeline using Albumentations.\nThe JSON file that contains the annotation information has the following keys:\ndict_keys([‘info’, ‘licenses’, ‘categories’, ‘images’, ‘annotations’])\nimages contains information about the image file whereas annotations contains information about the bounding boxes for each object in an image.\nFinally, categories contains keys that map to the type of chess pieces in the image.\nimage_list = json_file.get('images') anno_list = json_file.get('annotations') cat_list = json_file.get('categories')  image_list :\n[{'id': 0, 'license': 1, 'file_name': 'IMG_0317_JPG.rf.00207d2fe8c0a0f20715333d49d22b4f.jpg', 'height': 416, 'width': 416, 'date_captured': '2021-02-23T17:32:58+00:00'}, {'id': 1, 'license': 1, 'file_name': '5a8433ec79c881f84ef19a07dc73665d_jpg.rf.00544a8110f323e0d7721b3acf2a9e1e.jpg', 'height': 416, 'width': 416, 'date_captured': '2021-02-23T17:32:58+00:00'}, {'id': 2, 'license': 1, 'file_name': '675619f2c8078824cfd182cec2eeba95_jpg.rf.0130e3c26b1bf275bf240894ba73ed7c.jpg', 'height': 416, 'width': 416, 'date_captured': '2021-02-23T17:32:58+00:00'}, . . . .  anno_list :\n[{'id': 0, 'image_id': 0, 'category_id': 7, 'bbox': [220, 14, 18, 46.023746508293286], 'area': 828.4274371492792, 'segmentation':], 'iscrowd': 0}, {'id': 1, 'image_id': 1, 'category_id': 8, 'bbox': [187, 103, 22.686527154676014, 59.127992255841036], 'area': 1341.4088019136107, 'segmentation': [], 'iscrowd': 0}, {'id': 2, 'image_id': 2, 'category_id': 10, 'bbox': [203, 24, 24.26037020843023, 60.5], 'area': 1467.752397610029, 'segmentation': [], 'iscrowd': 0}, . . . .  cat_list :\n[{'id': 0, 'name': 'pieces', 'supercategory': 'none'}, {'id': 1, 'name': 'bishop', 'supercategory': 'pieces'}, {'id': 2, 'name': 'black-bishop', 'supercategory': 'pieces'}, {'id': 3, 'name': 'black-king', 'supercategory': 'pieces'}, {'id': 4, 'name': 'black-knight', 'supercategory': 'pieces'}, {'id': 5, 'name': 'black-pawn', 'supercategory': 'pieces'}, {'id': 6, 'name': 'black-queen', 'supercategory': 'pieces'}, {'id': 7, 'name': 'black-rook', 'supercategory': 'pieces'}, {'id': 8, 'name': 'white-bishop', 'supercategory': 'pieces'}, {'id': 9, 'name': 'white-king', 'supercategory': 'pieces'}, {'id': 10, 'name': 'white-knight', 'supercategory': 'pieces'}, {'id': 11, 'name': 'white-pawn', 'supercategory': 'pieces'}, {'id': 12, 'name': 'white-queen', 'supercategory': 'pieces'}, {'id': 13, 'name': 'white-rook', 'supercategory': 'pieces'}]  We have to alter the structure of these lists to create an efficient pipeline:\nnew_anno_dict = {} new_cat_dict = {} for item in cat_list: new_cat_dict[item['id']] = item['name'] for item in anno_list: img_id = item.get('image_id') if img_id not in new_anno_dict: temp_list = [] temp_list.append(item) new_anno_dict[img_id] = temp_list else: new_anno_dict.get(img_id).append(item)  Now, let’s create a simple augmentation pipeline that flips our image horizontally and adds a parameter for bounding boxes:\ntransform = A.Compose( [A.HorizontalFlip(p=0.5)], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']), )  Lastly, we will create a dataset similar to the Dataset class offered by Pytorch. To do this, we need to define a class that implements the methods __len__ and __getitem__.\nclass ImageDataset: def __init__(self, path, img_list, anno_dict, cat_dict, albumentations=None): self.path = path self.img_list = img_list self.anno_dict = anno_dict self.cat_dict = cat_dict self.albumentations = albumentations def __len__(self): return len(self.img_list) def __getitem__(self, idx): # Each image may have multiple objects thereby multiple bboxes bboxes = [item['bbox'] for item in self.anno_dict[int(idx)]] cat_ids = [item['category_id'] for item in self.anno_dict[int(idx)]] categories = [self.cat_dict[idx] for idx in cat_ids] image = self.img_list[idx] img = Image.open(f\u0026quot;{self.path}{image.get('file_name')}\u0026quot;) img = img.convert(\u0026quot;RGB\u0026quot;) img = np.array(img) if self.albumentations is not None: augmented = self.albumentations(image=img, bboxes=bboxes, category_ids=cat_ids) img = augmented[\u0026quot;image\u0026quot;] return { \u0026quot;image\u0026quot;: img, \u0026quot;bboxes\u0026quot;: augmented[\u0026quot;bboxes\u0026quot;], \u0026quot;category_ids\u0026quot;: augmented[\u0026quot;category_ids\u0026quot;], \u0026quot;category\u0026quot;: categories } # path is the path to the json_file and images dataset = ImageDataset(path, image_list, new_anno_dict, new_cat_dict, transform)  Here are some of the results while iterating on the custom dataset:\n           Thus, we can now easily pass this custom dataset to a data loader to train our model.\n Feature Extraction You may have heard of pre-trained models being used to train image classifiers and for other supervised learning tasks.\nBut, did you know that you can also use pre-trained models for feature extraction of images?\nIn short feature extraction is a form of dimensionality reduction where a large number of pixels are reduced to a more efficient representation.\nThis is primarily useful for unsupervised machine learning tasks such as reverse image search.\nLet’s try to extract features from images using Pytorch’s pre-trained models. To do this, we must first define our feature extractor class:\nclass ResnetFeatureExtractor(nn.Module): def __init__(self, model): super(ResnetFeatureExtractor, self).__init__() self.model = nn.Sequential(*model.children())[:-1] def forward(self, x): return self.model(x)  Note that in line 4, a new model is created with all of the layers of the original save for the last one. You will recall that the last layer in a neural network is a dense layer used for prediction outputs.\nHowever, since we are only interested in extracting features, we do not require this last layer. Hence, it is excluded.\nWe then utilize torchvision’s pre-trained resnet34 model by passing it to the ResnetFeatureExtractor constructor.\nLet’s use the famous CIFAR10 dataset (50000 images), and loop over it to extract the features.\n   cifar_dataset = CIFAR10(\u0026quot;./\u0026quot;, transform=transforms.ToTensor(), download=True) cifar_dataloader = DataLoader(cifar_dataset, batch_size=1, shuffle=True) feature_extractor.eval() feature_list = [] for _, data in enumerate(tqdm_notebook(cifar_dataloader)): inputs, labels = data with torch.no_grad(): extracted_features = feature_extractor(inputs) extracted_features = torch.flatten(extracted_features) feature_list.append(extracted_features)  We now have a list of 50000 image feature vectors with each feature vector of size 512 (output size of the penultimate layer of the original resnet model).\nprint(f\u0026quot;Number of feature vectors: {len(feature_list)}\u0026quot;) #50000 print(f\u0026quot;Number of feature vectors: {len(feature_list[0])}\u0026quot;) #512  Thus, this list of feature vectors can now be used by statistical learning models such as KNN to search for similar images.\nIf you have reached this far then thank you very much for reading this article! I hope you have a fantastic day ahead! 😄\n👉 Code used in the article\nUntil next time! ✋\n References:  https://www.cs.toronto.edu/~kriz/cifar.html https://www.practicaldeeplearning.ai/  ","date":1631750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638258570,"objectID":"acbee260398efab8405b827894f5f56c","permalink":"https://Sayar1106.github.io/post/fast_feature_engineering_image/","publishdate":"2021-09-16T00:00:00Z","relpermalink":"/post/fast_feature_engineering_image/","section":"post","summary":"Make your images more suitable to feed into ML systems","tags":[],"title":"Fast Feature Engineering in Python; Image Data","type":"post"},{"authors":["Sayar Banerjee"],"categories":[],"content":" An Algorithm must be seen to be believed — Donald Knuth\n Overview The science of Machine Learning can be broadly classified into two categories:\n Supervised learning Unsupervised learning  In this blog post, we will be implementing a popular unsupervised learning algorithm, k-means clustering.\nThis popular algorithm uses numerical distance measures to partition data into clusters. Algorithm Let’s say we have a bunch of observations and we want to segment “similar” observations together. We will use the following algorithm to achieve our goal.\n K-means algorithm Input: k (number of clusters), D (data points)\n Choose random k data points as initial clusters mean Associate each data point in D to the nearest centroid. This will divide the data into k clusters. Recompute centroids Repeat step 2 and step 3 until there are no more changes\nof cluster membership of the data points.  Let us look at the above algorithm in a bit more detail.\nWe first assign each data point to a cluster randomly. We then compute the cluster means for each group of clusters.\nAfter that, we proceed to compute the squared Euclidian distance between each point and cluster means. We then assign a cluster to each data point based on the smallest squared euclidian distance between that data point and the cluster means for each cluster.\n   The cluster means are then recomputed and we continue reassigning each data point based on the squared euclidian distance until no data point’s cluster assignment is changed.\nIf one were to ask a statistician, she/he might tell you that we are trying to minimize the within-cluster sum of squares (WCSS). Let’s now try to implement this algorithm in Python.\n    Implementation Though there are many library implementations of the k-means algorithm in Python, I decided to use only Numpy in order to provide an instructive approach. Numpy is a popular library in Python used for numerical computations.\nCode Walkthrough import numpy as np import tqdm import itertools import matplotlib.pyplot as plt class Kmeans: def __init__(self, k=3): self.k = k self.means = None self._cluster_ids = None  We first create a class called Kmeans and pass a single constructor argumentk to it. This argument is a hyperparameter. Hyperparameters are parameters that are set by the user before training the machine learning algorithm. In our case, this is the total number of clusters we wish to partition our data into. We also add two more attributes to the constructor, means which will store the cluster means and _cluster_ids which stores the id values of the clusters.\nimport numpy as np import tqdm import itertools import matplotlib.pyplot as plt class Kmeans: def __init__(self, k=3): self.k = k self.means = None self._cluster_ids = None @property def cluster_ids(self): return self._cluster_ids def _init_centroid(self, m): return np.random.randint(0, self.k, m)  We then create a method called cluster_ids which acts as a get method for our cluster ids. @property is a function decorator. To learn more about this, check out this article. Another method called _init_centroid is created to randomly assign each data point to a cluster.\nimport numpy as np import tqdm import itertools import matplotlib.pyplot as plt class Kmeans: def __init__(self, k=3): self.k = k self.means = None self._cluster_ids = None @property def cluster_ids(self): return self._cluster_ids def _init_centroid(self, m): return np.random.randint(0, self.k, m) def _cluster_means(self, X, clusters): m, n = X.shape[0], X.shape[1] # Extra column to store cluster ids temp = np.zeros((m, n + 1)) temp[:, :n], temp[:, n] = X, clusters result = np.zeros((self.k, n)) for i in range(self.k): subset = temp[np.where(temp[:, -1] == i), :n] if subset[0].shape[0] \u0026gt; 0: result[i] = np.mean(subset[0], axis=0) # Choose random data point if a cluster does not # have any data associated with it else: result[i] = X[np.random.choice(X.shape[0], 1, replace=True)] return result def _compute_cluster(self, x): # Computes closest means to a data point x return min(range(self.k), key=lambda i: np.linalg.norm(x - self.means[i])**2)  _cluster_means computes the means of our clusters. It accepts a Numpy array containing the data and another Numpy array which has the cluster ids as input. We use a temporary array temp to store our features and the cluster ids. We then compute the means of every data point in each cluster and return it as an array.\nNote that there could be some clusters which may not have any data (because we randomly assign clusters initially). Hence, if there is a cluster with no data, we randomly select an observation to be a part of that cluster.\n_compute_cluster is the method that determines which cluster’s means are closest to a data point. The np.linalg.norm() method does the computation for the euclidean distance. We square this to get the within-cluster sum of squares.\nimport numpy as np import tqdm import itertools import matplotlib.pyplot as plt class Kmeans: def __init__(self, k=3): self.k = k self.means = None self._cluster_ids = None @property def cluster_ids(self): return self._cluster_ids def _init_centroid(self, m): return np.random.randint(0, self.k, m) def _cluster_means(self, X, clusters): m, n = X.shape[0], X.shape[1] # Extra column to store cluster ids temp = np.zeros((m, n + 1)) temp[:, :n], temp[:, n] = X, clusters result = np.zeros((self.k, n)) for i in range(self.k): subset = temp[np.where(temp[:, -1] == i), :n] if subset[0].shape[0] \u0026gt; 0: result[i] = np.mean(subset[0], axis=0) # Choose random data point if a cluster does not # have any data associated with it else: result[i] = X[np.random.choice(X.shape[0], 1, replace=True)] return result def _compute_cluster(self, x): # Computes closest means to a data point x return min(range(self.k), key=lambda i: np.linalg.norm(x - self.means[i])**2) def fit(self, X, num_iterations=None): m = X.shape[0] # Initialize clusters initial_clusters = self._init_centroid(m) new_clusters = np.zeros(initial_clusters.shape) with tqdm.tqdm(itertools.count()) as t: for _ in t: # Compute cluster means self.means = self._cluster_means(X, initial_clusters) for i in range(m): # Assign new cluster ids new_clusters[i] = self._compute_cluster(X[i]) # Check for data points that have switched cluster ids. count_changed = (new_clusters != initial_clusters).sum() if count_changed == 0: break initial_clusters = new_clusters t.set_description(f\u0026quot;changed: {count_changed} / {X.shape[0]}\u0026quot;) self._cluster_ids = new_clusters  Finally, we create the fit method that orchestrates the clustering process.\nSteps in fit() method:\n We first initialize each observation to a cluster. We also create an array of zeroes to store the new cluster ids. We then use the function itertools.count() to create an infinite loop and compute cluster means. We then assign new cluster ids based on the squared distance between the cluster means and each data point. We then check if any data points changed clusters. If they did, then we use the new cluster ids to recompute the cluster means. Steps 2 to 4 are repeated until no data points change clusters.  And there you have it, folks! You have successfully created your own k means clustering class capable of clustering data. Here are some results on a few datasets:\nVisualizations         Choosing the value of k Since k is a hyperparameter, we have to have some methodology in order to pick an optimal value of k. One popular method is the elbow method.\nIn short, the elbow method plots a curve of the number of clusters vs percentage of explained variation. The curve produced by the elbow method is used by practitioners to determine the optimal number of clusters by following the law of diminishing returns.\nIf adding an extra cluster does not significantly improve the variation of k, we choose to stick to the current number of clusters.\n Tips and Optimizations Here are a couple of tips to ensure good clustering is obtained:\n Removing non-numeric features: Data may have non-numeric (categorical) features represented as numeric features. Instead of the numbers having some quantitative value, they might be used as labels for a group. For eg. if we are dealing with a population dataset, a column named “Gender” may have values 0 and 1 representing Male and Female. We must be careful in removing these features as they do not have any quantitative value and hence, will distort our algorithm’s notion of ‘distance’. **Feature Scaling:\n**Numeric data will have different ranges. A particular feature with a huge range may adversely impact our clustering objective function. The feature with the big range values will dominate the clustering process over other features. Hence, it is crucial to scale our data so that the contribution of each feature is proportional to the algorithm. **Better Initialization:\n**In our algorithm, we randomly assign the initial clusters to the data. Because of this inherent randomness, our algorithm may not always provide good clusters. There are a couple of ways by which the criterion for setting the initial clusters is improved. The k-means++ algorithm is a popular choice for this task. **Different Algorithms:\n**There are certain algorithms that are variants of the k-means algorithm which are more robust in handling certain constraints such as outliers. One such algorithm is the k-medoids. The k-medoids algorithm uses L1 distance instead of L2 distance (Euclidean distance). There are a bunch of other clustering algorithms that are useful for specific applications such as hierarchal clustering, density-based clustering, fuzzy clustering, etc.   Conclusion I hope all of you enjoyed this blog post. For more articles on Data Science check out my other posts on medium. Feel free to connect with me on LinkedIn. The code for this blog post is on my Github.\n References scikit-learn 0.23.1 documentation\nk-means clustering\nData Science from Scratch, 2nd Edition\n","date":1593734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593734400,"objectID":"6cc34c839ab934b825de86e5e79fd802","permalink":"https://Sayar1106.github.io/post/k-means-from-scratch/","publishdate":"2020-07-03T00:00:00Z","relpermalink":"/post/k-means-from-scratch/","section":"post","summary":"NumPy is all you need","tags":[],"title":"K-means Clustering from Scratch","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://Sayar1106.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Sayar Banerjee","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://Sayar1106.github.io/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://Sayar1106.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]