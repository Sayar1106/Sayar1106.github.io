<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Academic</title>
    <link>https://Sayar1106.github.io/</link>
      <atom:link href="https://Sayar1106.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Academic</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://Sayar1106.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Academic</title>
      <link>https://Sayar1106.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://Sayar1106.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://Sayar1106.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hot Dog or Not</title>
      <link>https://Sayar1106.github.io/project/hotdogornot/</link>
      <pubDate>Tue, 30 Nov 2021 12:56:20 +0530</pubDate>
      <guid>https://Sayar1106.github.io/project/hotdogornot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Heart Disease</title>
      <link>https://Sayar1106.github.io/project/heart-disease/</link>
      <pubDate>Tue, 30 Nov 2021 12:56:11 +0530</pubDate>
      <guid>https://Sayar1106.github.io/project/heart-disease/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Covid Dashboard</title>
      <link>https://Sayar1106.github.io/project/covid-dashboard/</link>
      <pubDate>Tue, 30 Nov 2021 12:56:02 +0530</pubDate>
      <guid>https://Sayar1106.github.io/project/covid-dashboard/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Awesome Data Scientist</title>
      <link>https://Sayar1106.github.io/project/awesome-data-scientist/</link>
      <pubDate>Tue, 30 Nov 2021 12:51:51 +0530</pubDate>
      <guid>https://Sayar1106.github.io/project/awesome-data-scientist/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hosting FastAPI with Saturn Cloud Deployments</title>
      <link>https://Sayar1106.github.io/post/fastapi-saturncloud/</link>
      <pubDate>Fri, 19 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://Sayar1106.github.io/post/fastapi-saturncloud/</guid>
      <description>&lt;p&gt;Hi all, this article will explore the process of deploying a FastAPI application on Saturn Cloud. FastAPI is a robust web framework for building APIs with the Python language. &lt;a href=&#34;https://saturncloud.io?utm_source=Sayar&amp;#43;Medium&amp;amp;utm_medium=FastAPI&amp;#43;Blog&amp;amp;utm_campaign=FastAPI&amp;#43;Blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Saturn Cloud&lt;/a&gt; is a platform dedicated to scaling Machine Learning and Big Data pipelines and more.&lt;/p&gt;
&lt;p&gt;The model will predict median house prices in California. Let&amp;rsquo;s jump right into it.&lt;/p&gt;
&lt;h3 id=&#34;resources&#34;&gt;Resources&lt;/h3&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://github.com/Sayar1106/cali-house-prices-estimator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://fastapi.tiangolo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FastAPI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-learn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://joblib.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Joblib&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;data-exploration&#34;&gt;Data Exploration&lt;/h3&gt;
&lt;p&gt;The dataset I will use for training our machine learning model is called &amp;ldquo;California Housing Prices.&amp;rdquo; It can be found &lt;a href=&#34;https://www.kaggle.com/camnugent/california-housing-prices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The contents of our data are as follows:&lt;/p&gt;
&lt;p&gt;The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned, the data isn&amp;rsquo;t clean, so there are some preprocessing steps required! The columns are as follows, and their names are pretty self-explanatory:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;longitude&lt;/li&gt;
&lt;li&gt;latitude&lt;/li&gt;
&lt;li&gt;housing_median_age&lt;/li&gt;
&lt;li&gt;total_rooms&lt;/li&gt;
&lt;li&gt;total_bedrooms&lt;/li&gt;
&lt;li&gt;population&lt;/li&gt;
&lt;li&gt;households&lt;/li&gt;
&lt;li&gt;median_income&lt;/li&gt;
&lt;li&gt;median_house_ value&lt;/li&gt;
&lt;li&gt;ocean_proximity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On doing a rudimentary exploration of the dataset, I found the following:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_1.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;A correlation plot between all the numerical features shows us the following:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_2.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;According to the graph, most numerical features have very little correlation with median_house_value except median_income, which seems to have a strong positive correlation of around 0.68.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;data-cleaningfeature-engineering&#34;&gt;Data Cleaning/Feature Engineering&lt;/h3&gt;
&lt;p&gt;Since the total_bedrooms feature had missing values, I had to impute it. For simplicity, I chose the median as the metric to impute the feature.&lt;/p&gt;
&lt;p&gt;Additionally, two new features were engineered, namely, &amp;ldquo;rooms_per_households&amp;rdquo; and &amp;ldquo;population_per_household.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_3.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;training-themodel&#34;&gt;Training the¬†Model&lt;/h3&gt;
&lt;p&gt;Our repository looks like this:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_4.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The requirements.txt file contains our dependencies. It is crucial to have all the dependencies added to this file as it will be used during our deployment.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_5.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The file src/main.py contains our training script. Let us take a look at some of the essential functions in the script.&lt;/p&gt;
&lt;p&gt;Our training model pipeline is relatively standard. There is just one categorical column (ocean_proximity). For the other numerical columns, I applied a standard scaler. The ColumnTransformer estimator helps to facilitate feature transformations on heterogeneous data.&lt;/p&gt;
&lt;p&gt;As for the model, I chose the Random Forest algorithm. I created the pipeline using scikit-learn&amp;rsquo;s Pipeline class.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_6.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;I used joblib to save our model. Since the model file was quite large (&amp;gt;100Mb), I decided to store it in AWS S3. The model&amp;rsquo;s R¬≤ score was around 0.81, and the RMSE was around 49k.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;setting-up-fastapi-server-andfrontend&#34;&gt;Setting up FastAPI Server and¬†Frontend&lt;/h3&gt;
&lt;p&gt;As you may have guessed, app/main.py contains our code for the server. Since the model is stored in AWS, I used boto3 to download a local copy to the server.&lt;/p&gt;
&lt;p&gt;If your bucket and file are private, you may need to set up authentication to access it on Saturn Cloud. You can do it by following this &lt;a href=&#34;https://saturncloud.io/docs/using-saturn-cloud/connect_data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I wrote a simple function to load our model from AWS:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_7.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The variables BUCKET_NAME and FILE_NAME are self-explanatory. LOCAL_PATH is the path to where the model will be copied locally.&lt;/p&gt;
&lt;p&gt;I also defined global variables for the app, model, and templates.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_8.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;homepage&#34;&gt;Homepage&lt;/h4&gt;
&lt;p&gt;Since I&amp;rsquo;m creating an application, it&amp;rsquo;s essential to have a homepage to serve as an interface for the model server.&lt;/p&gt;
&lt;p&gt;I created a homepage for the app so that users can enter values for each of the features. To render the page, I used Jinja2Templates, which is provided out of the box by FastAPI templates.TemplateResponse renders our landing page titled &amp;ldquo;index.html.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_9.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;index.html contains a form that will serve as the frontend for our application. The body of the page looks like this:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_10.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;If you look closely at the form tag, you will see that the action attribute is set to &amp;ldquo;/submitform&amp;rdquo; and the request method is a POST request.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_11.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Our FastAPI server needs to have a method that handles the form data. This method needs to be decorated by app.post(&amp;quot;/submitform&amp;quot;) to handle the request appropriately.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_12.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;You will notice that each of the variables is set as form parameters using Form. This class tells FastAPI that each variable&amp;rsquo;s input is being received from a form.&lt;/p&gt;
&lt;p&gt;You will also notice that line 26 has a method called predict. This method is actually where the model pipeline is fed the input from the form using the appropriate format. Since the pipeline can only receive input from a data frame, I first convert the data into a data frame. I then created the features as part of the feature engineering process. Finally, I return the model&amp;rsquo;s predictions.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_13.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Once I had the price prediction, I used templates.TemplateResponse again to return a page called result.html. Along with &amp;ldquo;request&amp;rdquo;, I also passed &amp;ldquo;price&amp;rdquo; through the TemplateResponse method. Finally, I rendered the price on the body of result.html.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_14.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;deploying-to-saturncloud&#34;&gt;Deploying to Saturn¬†Cloud&lt;/h3&gt;
&lt;p&gt;Before setting up the deployment, I pushed all of the code to Github. To deploy it, you must have your repository connected to Saturn Cloud. To do so, you can follow this &lt;a href=&#34;https://saturncloud.io/docs/using-saturn-cloud/gitrepo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once your repo is connected, head over to resources and select &amp;ldquo;New Deployment&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_15.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;After this, you will be greeted with a form:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_16.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are a few things to note when filling out the form. For instance, the &amp;ldquo;Command&amp;rdquo; is what the deployment will run to start your application.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_17.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Note that Saturn Cloud requires your applications to listen using port 8000.&lt;/p&gt;
&lt;p&gt;Also, note the Extra Packages header. This is the script that will be used to install additional packages before the command is run. Since Saturn Cloud&amp;rsquo;s default image does not have certain libraries like FastAPI and Uvicorn, pass &amp;ldquo;-r requirements.txt&amp;rdquo; to the text box.&lt;/p&gt;
&lt;p&gt;This ensures that the script &amp;ldquo;`pip install -r requirements.txt` &amp;ldquo;is run before startup, containing dependencies for the additional packages.&lt;/p&gt;
&lt;p&gt;Note that you can also write the individual names of each package in this section to install them.&lt;/p&gt;
&lt;p&gt;Once you hit the Create button, your deployment will be created. Click on it and add your Github repo to the deployment. Ensure that you add the path to the Github resource to your working directory. Once that is done, click the green arrow to start the deployment.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_18.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Once your deployment is ready, click on the public URL. You should see a page like this:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_19.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Once you fill out the form, you will see a page with the predicted price:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fastapi_saturn/img_20.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Note that I used the last example of my test set as input. The actual median house price was $133000, so the model did a reasonably good job! üòÄ&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;üëâ&lt;/strong&gt;  &lt;a href=&#34;https://github.com/Sayar1106/cali-house-prices-estimator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Link to the Github directory&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Congratulations! You have successfully learned how to deploy a FastAPI model on &lt;a href=&#34;https://saturncloud.io?utm_source=Sayar&amp;#43;Medium&amp;amp;utm_medium=FastAPI&amp;#43;Blog&amp;amp;utm_campaign=FastAPI&amp;#43;Blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Saturn Cloud&lt;/a&gt;! If you&amp;rsquo;re curious about using their environment, they offer 30 free hours a month for data scientists and teams. I hope you enjoyed reading this article. Until next time! ‚úã&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fast Feature Engineering in Python; Image Data</title>
      <link>https://Sayar1106.github.io/post/fast_feature_engineering_image/</link>
      <pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://Sayar1106.github.io/post/fast_feature_engineering_image/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;‚ÄúFinding patterns is easy in any kind of data-rich environment; that‚Äôs what mediocre gamblers do. The key is in determining whether the patterns represent noise or signal.‚Äù&lt;br&gt;
‚Äï &lt;strong&gt;Nate¬†Silver&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This article is part 2 of my ‚ÄúFast Feature Engineering‚Äù series. If you have not read my first article which talks about tabular data, then I request you to check it out here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/fast-feature-engineering-in-python-tabular-data-d050b68bb178&#34; title=&#34;https://towardsdatascience.com/fast-feature-engineering-in-python-tabular-data-d050b68bb178&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Fast Feature Engineering in Python: Tabular Data&lt;/strong&gt;&lt;/a&gt;&lt;a href=&#34;https://towardsdatascience.com/fast-feature-engineering-in-python-tabular-data-d050b68bb178&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This article will look at some of the best practices to follow when performing image processing as part of our machine learning workflow.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;libraries&#34;&gt;Libraries&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random  
from PIL import Image  
import cv2  
import numpy as np  
from matplotlib import pyplot as plt  
import json  
import albumentations as A  
import torch  
import torchvision.models as models  
import torchvision.transforms as transforms  
import torch.nn as nn  
from tqdm import tqdm_notebook  
from torch.utils.data import DataLoader  
from torchvision.datasets import CIFAR10
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;resizescale-images&#34;&gt;Resize/Scale Images&lt;/h3&gt;
&lt;p&gt;Resizing is the most fundamental transformation done by deep learning practitioners in the field. The primary reason for doing this is to ensure that the input received by our deep learning system is &lt;strong&gt;consistent&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Another reason for resizing is to &lt;strong&gt;reduce the number of parameters&lt;/strong&gt; in the model. Smaller dimensions signify a smaller neural network and hence, saves us the time and computation power required to train our model.&lt;/p&gt;
&lt;h4 id=&#34;_what-about-the-loss-of-information_&#34;&gt;&lt;strong&gt;&lt;em&gt;What about the loss of information?&lt;/em&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Some information is indeed &lt;strong&gt;lost&lt;/strong&gt; when you resize down from a larger image. However, depending on your task, you can choose how much information you‚Äôre willing to sacrifice for training time and compute resources.&lt;/p&gt;
&lt;p&gt;For example, an &lt;a href=&#34;https://en.wikipedia.org/wiki/Object_detection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;object detection task&lt;/strong&gt;&lt;/a&gt; will require you to maintain the image&amp;rsquo;s aspect ratio since the goal is to detect the exact position of objects.&lt;/p&gt;
&lt;p&gt;In contrast, an image classification task may require you to resize all images down to a specified size (224 x 224 is a good rule of thumb).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_1.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;After resizing our image looks like this:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_2.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;_why-perform-imagescaling_&#34;&gt;&lt;em&gt;Why perform image¬†scaling?&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Similar to tabular data, scaling images for classification tasks can help our deep learning model&amp;rsquo;s learning rate to converge to the minima better.&lt;/p&gt;
&lt;p&gt;Scaling ensures that a particular dimension does not dominate others. I found a fantastic answer on StackExchange regarding this. You can read it &lt;a href=&#34;https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One type of feature scaling is the process of &lt;strong&gt;standardizing&lt;/strong&gt; our pixel values. We do this by subtracting the mean of each channel from its pixel value and then divide it via standard deviation.&lt;/p&gt;
&lt;p&gt;This is a popular choice of feature engineering when training models for classification tasks.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mean = np.mean(img_resized, axis=(1,2), keepdims=True)
std = np.std(img_resized, axis=(1,2), keepdims=True)
img_std = (img_resized - mean) / std
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note: Like resizing, one may not want to do image scaling when performing object detection and image generation tasks.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The example code above demonstrates the process of scaling an image via standardization. There are other forms of scaling such as &lt;strong&gt;centering&lt;/strong&gt; and &lt;strong&gt;normalization&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;augmentations-classification&#34;&gt;Augmentations (Classification)&lt;/h3&gt;
&lt;p&gt;The primary motivation behind augmenting images is due to the appreciable data requirement for computer vision tasks. Often, obtaining enough images for training can prove to be challenging for a multitude of reasons.&lt;/p&gt;
&lt;p&gt;Image augmentation enables us to create new training samples by slightly modifying the original ones.&lt;/p&gt;
&lt;p&gt;In this example, we will look at how to apply vanilla augmentations for a classification task. We can use the out of the box implementations of the &lt;strong&gt;Albumentations&lt;/strong&gt; library to do this:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_3.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_4.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_5.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;By applying image augmentations, our deep learning models can generalize better to the task (avoid overfitting), thereby increasing its predictive power on unseen data.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;augmentations-object-detection&#34;&gt;Augmentations (Object Detection)&lt;/h3&gt;
&lt;p&gt;The Albumentations library can also be used to create augmentations for other tasks such as object detections. Object detection requires us to create bounding boxes around the object of interest.&lt;/p&gt;
&lt;p&gt;Working with raw data can prove to be challenging when trying to annotate images with the coordinates for the bounding boxes.&lt;/p&gt;
&lt;p&gt;Fortunately, there are many publicly and freely available datasets that we can use to create an augmentation pipeline for object detection. One such dataset is the &lt;a href=&#34;https://public.roboflow.com/object-detection/chess-full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Chess Dataset&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The dataset contains 606 images of chess pieces on a chessboard.&lt;/p&gt;
&lt;p&gt;Along with the images, a JSON file is provided that contains all the information pertaining to the bounding boxes for each chess piece in a single image.&lt;/p&gt;
&lt;p&gt;By writing a simple function, we can visualize the data after the augmentation is applied:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with open(&amp;quot;_annotations.coco.json&amp;quot;) as f:
    json_file = json.load(f)
    
x_min, y_min, w, h = json_file[&#39;annotations&#39;][0][&#39;bbox&#39;]
x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)

def visualize_bbox(img, bbox, class_name, color=(0, 255, 0), thickness=2):
    x_min, y_min, w, h = bbox
    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)

    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)

    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    
    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)
    cv2.putText(
        img,
        text=class_name,
        org=(x_min, y_min - int(0.3 * text_height)),
        fontFace=cv2.FONT_HERSHEY_SIMPLEX,
        fontScale=0.35, 
        color=(255, 255, 255), 
        lineType=cv2.LINE_AA,
    )
    return img
  
bbox_img = visualize_bbox(np.array(img), 
                          json_file[&#39;annotations&#39;][0][&#39;bbox&#39;], 
                          class_name=json_file[&#39;categories&#39;][0][&#39;name&#39;])

Image.fromarray(bbox_img)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/Users/Banner/Downloads/medium-export-aa5b5fa1b4f15ba326f851375de5c386499a5652f183eac85ab56b6ca8924b20/posts/md_1638090489769/img/1__MFakz3EYf73afrl__aT3S2A.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_6.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Now, let‚Äôs try to create an augmentation pipeline using Albumentations.&lt;/p&gt;
&lt;p&gt;The JSON file that contains the annotation information has the following keys:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dict_keys([‚Äòinfo‚Äô, ‚Äòlicenses‚Äô, ‚Äòcategories‚Äô, ‚Äòimages‚Äô, ‚Äòannotations‚Äô])&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;images&lt;/code&gt; contains information about the image file whereas &lt;code&gt;annotations&lt;/code&gt; contains information about the bounding boxes for each object in an image.&lt;/p&gt;
&lt;p&gt;Finally, &lt;code&gt;categories&lt;/code&gt; contains keys that map to the type of chess pieces in the image.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;image_list = json_file.get(&#39;images&#39;)  
anno_list = json_file.get(&#39;annotations&#39;)  
cat_list = json_file.get(&#39;categories&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;image_list&lt;/code&gt;¬†:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[{&#39;id&#39;: 0,  
  &#39;license&#39;: 1,  
  &#39;file_name&#39;: &#39;IMG_0317_JPG.rf.00207d2fe8c0a0f20715333d49d22b4f.jpg&#39;,  
  &#39;height&#39;: 416,  
  &#39;width&#39;: 416,  
  &#39;date_captured&#39;: &#39;2021-02-23T17:32:58+00:00&#39;},  
 {&#39;id&#39;: 1,  
  &#39;license&#39;: 1,  
  &#39;file_name&#39;: &#39;5a8433ec79c881f84ef19a07dc73665d_jpg.rf.00544a8110f323e0d7721b3acf2a9e1e.jpg&#39;,  
  &#39;height&#39;: 416,  
  &#39;width&#39;: 416,  
  &#39;date_captured&#39;: &#39;2021-02-23T17:32:58+00:00&#39;},  
 {&#39;id&#39;: 2,  
  &#39;license&#39;: 1,  
  &#39;file_name&#39;: &#39;675619f2c8078824cfd182cec2eeba95_jpg.rf.0130e3c26b1bf275bf240894ba73ed7c.jpg&#39;,  
  &#39;height&#39;: 416,  
  &#39;width&#39;: 416,  
  &#39;date_captured&#39;: &#39;2021-02-23T17:32:58+00:00&#39;},  
.  
.  
.  
.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;anno_list&lt;/code&gt;¬†:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[{&#39;id&#39;: 0,  
  &#39;image_id&#39;: 0,  
  &#39;category_id&#39;: 7,  
  &#39;bbox&#39;: [220, 14, 18, 46.023746508293286],  
  &#39;area&#39;: 828.4274371492792,  
  &#39;segmentation&#39;:],  
  &#39;iscrowd&#39;: 0},  
 {&#39;id&#39;: 1,  
  &#39;image_id&#39;: 1,  
  &#39;category_id&#39;: 8,  
  &#39;bbox&#39;: [187, 103, 22.686527154676014, 59.127992255841036],  
  &#39;area&#39;: 1341.4088019136107,  
  &#39;segmentation&#39;: [],  
  &#39;iscrowd&#39;: 0},  
 {&#39;id&#39;: 2,  
  &#39;image_id&#39;: 2,  
  &#39;category_id&#39;: 10,  
  &#39;bbox&#39;: [203, 24, 24.26037020843023, 60.5],  
  &#39;area&#39;: 1467.752397610029,  
  &#39;segmentation&#39;: [],  
  &#39;iscrowd&#39;: 0},  
.  
.  
.  
.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;cat_list&lt;/code&gt;¬†:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[{&#39;id&#39;: 0, &#39;name&#39;: &#39;pieces&#39;, &#39;supercategory&#39;: &#39;none&#39;},  
 {&#39;id&#39;: 1, &#39;name&#39;: &#39;bishop&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 2, &#39;name&#39;: &#39;black-bishop&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 3, &#39;name&#39;: &#39;black-king&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 4, &#39;name&#39;: &#39;black-knight&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 5, &#39;name&#39;: &#39;black-pawn&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 6, &#39;name&#39;: &#39;black-queen&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 7, &#39;name&#39;: &#39;black-rook&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 8, &#39;name&#39;: &#39;white-bishop&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 9, &#39;name&#39;: &#39;white-king&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 10, &#39;name&#39;: &#39;white-knight&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 11, &#39;name&#39;: &#39;white-pawn&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 12, &#39;name&#39;: &#39;white-queen&#39;, &#39;supercategory&#39;: &#39;pieces&#39;},  
 {&#39;id&#39;: 13, &#39;name&#39;: &#39;white-rook&#39;, &#39;supercategory&#39;: &#39;pieces&#39;}]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have to alter the structure of these lists to create an efficient pipeline:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_anno_dict = {}
new_cat_dict = {}

for item in cat_list:
    new_cat_dict[item[&#39;id&#39;]] = item[&#39;name&#39;]
    

for item in anno_list:
    img_id = item.get(&#39;image_id&#39;)
    if img_id not in new_anno_dict:
        temp_list = []
        temp_list.append(item)
        new_anno_dict[img_id] = temp_list
    else:
        new_anno_dict.get(img_id).append(item)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let‚Äôs create a simple augmentation pipeline that flips our image horizontally and adds a parameter for bounding boxes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transform = A.Compose(
    [A.HorizontalFlip(p=0.5)],
    bbox_params=A.BboxParams(format=&#39;coco&#39;, label_fields=[&#39;category_ids&#39;]),
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we will create a dataset similar to the &lt;a href=&#34;https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Dataset class&lt;/strong&gt;&lt;/a&gt; offered by Pytorch. To do this, we need to define a class that implements the methods &lt;code&gt;__len__&lt;/code&gt; and &lt;code&gt;__getitem__&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class ImageDataset:
    def __init__(self, path, img_list, anno_dict, cat_dict, albumentations=None):
        self.path = path
        self.img_list = img_list
        self.anno_dict = anno_dict
        self.cat_dict = cat_dict
        self.albumentations = albumentations
    
    def __len__(self):
        return len(self.img_list)
    
    def __getitem__(self, idx):
        # Each image may have multiple objects thereby multiple bboxes
        bboxes = [item[&#39;bbox&#39;] for item in self.anno_dict[int(idx)]]
        cat_ids = [item[&#39;category_id&#39;] for item in self.anno_dict[int(idx)]]
        categories = [self.cat_dict[idx] for idx in cat_ids]
        image = self.img_list[idx]
        img = Image.open(f&amp;quot;{self.path}{image.get(&#39;file_name&#39;)}&amp;quot;)
        img = img.convert(&amp;quot;RGB&amp;quot;)
        img = np.array(img)
        if self.albumentations is not None:
            augmented = self.albumentations(image=img, bboxes=bboxes, category_ids=cat_ids)
            img = augmented[&amp;quot;image&amp;quot;]
        return {
            &amp;quot;image&amp;quot;: img,
            &amp;quot;bboxes&amp;quot;: augmented[&amp;quot;bboxes&amp;quot;],
            &amp;quot;category_ids&amp;quot;: augmented[&amp;quot;category_ids&amp;quot;],
            &amp;quot;category&amp;quot;: categories
        }

 # path is the path to the json_file and images
dataset = ImageDataset(path, image_list, new_anno_dict, new_cat_dict, transform)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are some of the results while iterating on the custom dataset:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_7.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_8.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_9.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_10.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_11.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Thus, we can now easily pass this custom dataset to a data loader to train our model.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;feature-extraction&#34;&gt;Feature Extraction&lt;/h3&gt;
&lt;p&gt;You may have heard of pre-trained models being used to train image classifiers and for other supervised learning tasks.&lt;/p&gt;
&lt;p&gt;But, did you know that you can also use pre-trained models for feature extraction of images?&lt;/p&gt;
&lt;p&gt;In short feature extraction is a form of dimensionality reduction where a large number of pixels are reduced to a more efficient representation.&lt;/p&gt;
&lt;p&gt;This is primarily useful for unsupervised machine learning tasks such as reverse image search.&lt;/p&gt;
&lt;p&gt;Let‚Äôs try to extract features from images using Pytorch‚Äôs pre-trained models. To do this, we must first define our feature extractor class:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class ResnetFeatureExtractor(nn.Module):
    def __init__(self, model):
        super(ResnetFeatureExtractor, self).__init__()
        self.model = nn.Sequential(*model.children())[:-1]
    def forward(self, x):
        return self.model(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in line 4, a new model is created with all of the layers of the original save for the last one. You will recall that the last layer in a neural network is a dense layer used for prediction outputs.&lt;/p&gt;
&lt;p&gt;However, since we are only interested in extracting features, we do not require this last layer. Hence, it is excluded.&lt;/p&gt;
&lt;p&gt;We then utilize torchvision‚Äôs pre-trained &lt;code&gt;resnet34&lt;/code&gt; model by passing it to the &lt;code&gt;ResnetFeatureExtractor&lt;/code&gt; constructor.&lt;/p&gt;
&lt;p&gt;Let‚Äôs use the famous &lt;a href=&#34;https://paperswithcode.com/dataset/cifar-10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;CIFAR10 dataset&lt;/strong&gt;&lt;/a&gt; (50000 images), and loop over it to extract the features.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/fast_feature_engineering/img_12.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cifar_dataset = CIFAR10(&amp;quot;./&amp;quot;, transform=transforms.ToTensor(), download=True)
cifar_dataloader = DataLoader(cifar_dataset, batch_size=1, shuffle=True)

feature_extractor.eval()
feature_list = []

for _, data in enumerate(tqdm_notebook(cifar_dataloader)):
    inputs, labels = data
    with torch.no_grad():
        extracted_features = feature_extractor(inputs)
    extracted_features = torch.flatten(extracted_features)
    feature_list.append(extracted_features)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a list of 50000 image feature vectors with each feature vector of size 512 (output size of the penultimate layer of the original resnet model).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(f&amp;quot;Number of feature vectors: {len(feature_list)}&amp;quot;) #50000  
print(f&amp;quot;Number of feature vectors: {len(feature_list[0])}&amp;quot;) #512
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, this list of feature vectors can now be used by statistical learning models such as KNN to search for similar images.&lt;/p&gt;
&lt;p&gt;If you have reached this far then thank you very much for reading this article! I hope you have a fantastic day ahead! üòÑ&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;üëâ&lt;/strong&gt; &lt;a href=&#34;https://github.com/Sayar1106/TowardsDataSciencecodefiles/tree/master/fast_feature_engineering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Code used in the article&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Until next time! ‚úã&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;references&#34;&gt;References:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cs.toronto.edu/~kriz/cifar.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.practicaldeeplearning.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.practicaldeeplearning.ai/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>K-means Clustering from Scratch</title>
      <link>https://Sayar1106.github.io/post/k-means-from-scratch/</link>
      <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://Sayar1106.github.io/post/k-means-from-scratch/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;An Algorithm must be seen to be believed‚Ää‚Äî‚ÄäDonald¬†Knuth&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;The science of Machine Learning can be broadly classified into two categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Supervised_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Supervised learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Unsupervised_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unsupervised learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this blog post, we will be implementing a popular unsupervised learning algorithm, k-means clustering.&lt;/p&gt;
&lt;h2 id=&#34;this-popular-algorithm-uses-numerical-distance-measures-to-partition-data-into-clusters&#34;&gt;This popular algorithm uses numerical distance measures to partition data into clusters.&lt;/h2&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;Let‚Äôs say we have a bunch of observations and we want to segment ‚Äúsimilar‚Äù observations together. We will use the following algorithm to achieve our goal.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;_k-means-algorithm_&#34;&gt;&lt;em&gt;K-means algorithm&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Input: k (number of clusters), D (data points)&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Choose random k data points as initial clusters mean&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Associate each data point in D to the nearest centroid.¬†&lt;br&gt;
This will divide the data into k clusters.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Recompute centroids&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Repeat step 2 and step 3 until there are no more changes&lt;br&gt;
of cluster membership of the data points.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let us look at the above algorithm in a bit more detail.&lt;/p&gt;
&lt;p&gt;We first assign each data point to a cluster randomly. We then compute the cluster means for each group of clusters.&lt;/p&gt;
&lt;p&gt;After that, we proceed to compute the squared &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_distance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Euclidian distance&lt;/a&gt; between each point and cluster means. We then assign a cluster to each data point based on the smallest squared euclidian distance between that data point and the cluster means for each cluster.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/k_means_from_scratch/img_1.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

The cluster means are then recomputed and we continue reassigning each data point based on the squared euclidian distance until no data point‚Äôs cluster assignment is changed.&lt;/p&gt;
&lt;p&gt;If one were to ask a statistician, she/he might tell you that we are trying to minimize the &lt;strong&gt;within-cluster sum of squares (WCSS).&lt;/strong&gt; Let‚Äôs now try to implement this algorithm in Python.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/k_means_from_scratch/img_2.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;Though there are many library implementations of the k-means algorithm in Python, I decided to use only Numpy in order to provide an instructive approach. Numpy is a popular library in Python used for numerical computations.&lt;/p&gt;
&lt;h4 id=&#34;code-walkthrough&#34;&gt;Code Walkthrough&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import tqdm
import itertools
import matplotlib.pyplot as plt


class Kmeans:
    def __init__(self, k=3):
        self.k = k
        self.means = None
        self._cluster_ids = None
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first create a class called &lt;code&gt;Kmeans&lt;/code&gt; and pass a single constructor argument&lt;code&gt;k&lt;/code&gt; to it. This argument is a &lt;strong&gt;hyperparameter&lt;/strong&gt;. Hyperparameters are parameters that are set by the user before training the machine learning algorithm. In our case, this is the total number of clusters we wish to partition our data into. We also add two more attributes to the constructor, &lt;code&gt;means&lt;/code&gt; which will store the cluster means and &lt;code&gt;_cluster_ids&lt;/code&gt; which stores the id values of the clusters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import tqdm
import itertools
import matplotlib.pyplot as plt


class Kmeans:
    def __init__(self, k=3):
        self.k = k
        self.means = None
        self._cluster_ids = None

    @property
    def cluster_ids(self):
        return self._cluster_ids

    def _init_centroid(self, m):
        return np.random.randint(0, self.k, m)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then create a method called &lt;code&gt;cluster_ids&lt;/code&gt; which acts as a get method for our cluster ids. &lt;code&gt;@property&lt;/code&gt; is a function decorator. To learn more about this, check out &lt;a href=&#34;https://www.programiz.com/python-programming/property&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; article. Another method called &lt;code&gt;_init_centroid&lt;/code&gt; is created to &lt;strong&gt;randomly assign&lt;/strong&gt; each data point to a cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import tqdm
import itertools
import matplotlib.pyplot as plt


class Kmeans:
    def __init__(self, k=3):
        self.k = k
        self.means = None
        self._cluster_ids = None

    @property
    def cluster_ids(self):
        return self._cluster_ids

    def _init_centroid(self, m):
        return np.random.randint(0, self.k, m)

    def _cluster_means(self, X, clusters):
        m, n = X.shape[0], X.shape[1]
        # Extra column to store cluster ids
        temp = np.zeros((m, n + 1))
        temp[:, :n], temp[:, n] = X, clusters
        result = np.zeros((self.k, n))
        for i in range(self.k):
            subset = temp[np.where(temp[:, -1] == i), :n]
            if subset[0].shape[0] &amp;gt; 0:
                result[i] = np.mean(subset[0], axis=0)
            # Choose random data point if a cluster does not 
            # have any data associated with it
            else:
                result[i] = X[np.random.choice(X.shape[0], 1, replace=True)]

        return result

    def _compute_cluster(self, x):
        # Computes closest means to a data point x
        return min(range(self.k), key=lambda i: np.linalg.norm(x - self.means[i])**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;_cluster_means&lt;/code&gt; computes the means of our clusters. It accepts a Numpy array containing the data and another Numpy array which has the cluster ids as input. We use a temporary array &lt;code&gt;temp&lt;/code&gt; to store our features and the cluster ids. We then compute the means of every data point in each cluster and return it as an array.&lt;/p&gt;
&lt;p&gt;Note that there could be some clusters which may not have any data (because we randomly assign clusters initially). Hence, if there is a cluster with no data, we randomly select an observation to be a part of that cluster.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;_compute_cluster&lt;/code&gt; is the method that determines which cluster‚Äôs means are closest to a data point. The &lt;code&gt;np.linalg.norm()&lt;/code&gt; method does the computation for the &lt;strong&gt;euclidean distance&lt;/strong&gt;. We square this to get the &lt;strong&gt;within-cluster sum of squares.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import tqdm
import itertools
import matplotlib.pyplot as plt


class Kmeans:
    def __init__(self, k=3):
        self.k = k
        self.means = None
        self._cluster_ids = None

    @property
    def cluster_ids(self):
        return self._cluster_ids

    def _init_centroid(self, m):
        return np.random.randint(0, self.k, m)

    def _cluster_means(self, X, clusters):
        m, n = X.shape[0], X.shape[1]
        # Extra column to store cluster ids
        temp = np.zeros((m, n + 1))
        temp[:, :n], temp[:, n] = X, clusters
        result = np.zeros((self.k, n))
        for i in range(self.k):
            subset = temp[np.where(temp[:, -1] == i), :n]
            if subset[0].shape[0] &amp;gt; 0:
                result[i] = np.mean(subset[0], axis=0)
            # Choose random data point if a cluster does not 
            # have any data associated with it
            else:
                result[i] = X[np.random.choice(X.shape[0], 1, replace=True)]

        return result

    def _compute_cluster(self, x):
        # Computes closest means to a data point x
        return min(range(self.k), key=lambda i: np.linalg.norm(x - self.means[i])**2)

    def fit(self, X, num_iterations=None):
        m = X.shape[0]
        # Initialize clusters
        initial_clusters = self._init_centroid(m)
        new_clusters = np.zeros(initial_clusters.shape)
        with tqdm.tqdm(itertools.count()) as t:
            for _ in t:
                # Compute cluster means
                self.means = self._cluster_means(X, initial_clusters)
                for i in range(m):
                    # Assign new cluster ids
                    new_clusters[i] = self._compute_cluster(X[i])
                # Check for data points that have switched cluster ids.
                count_changed = (new_clusters != initial_clusters).sum()
                if count_changed == 0:
                    break
                initial_clusters = new_clusters
                t.set_description(f&amp;quot;changed: {count_changed} / {X.shape[0]}&amp;quot;)

        self._cluster_ids = new_clusters
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we create the fit method that orchestrates the clustering process.&lt;/p&gt;
&lt;p&gt;Steps in &lt;code&gt;fit()&lt;/code&gt; method:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We first initialize each observation to a cluster. We also create an array of zeroes to store the new cluster ids.&lt;/li&gt;
&lt;li&gt;We then use the function &lt;code&gt;itertools.count()&lt;/code&gt; to create an infinite loop and compute cluster means.&lt;/li&gt;
&lt;li&gt;We then assign new cluster ids based on the squared distance between the cluster means and each data point.&lt;/li&gt;
&lt;li&gt;We then check if any data points changed clusters. If they did, then we use the new cluster ids to recompute the cluster means.&lt;/li&gt;
&lt;li&gt;Steps 2 to 4 are repeated until no data points change clusters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And there you have it, folks! You have successfully created your own k means clustering class capable of clustering data. Here are some results on a few datasets:&lt;/p&gt;
&lt;h4 id=&#34;visualizations&#34;&gt;Visualizations&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/k_means_from_scratch/img_3.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/k_means_from_scratch/img_4.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://Sayar1106.github.io/posts_img/k_means_from_scratch/img_5.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;choosing-the-value-ofk&#34;&gt;Choosing the value of¬†k&lt;/h3&gt;
&lt;p&gt;Since k is a hyperparameter, we have to have some methodology in order to pick an optimal value of k. One popular method is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Elbow_method_%28clustering%29#:~:text=In%20cluster%20analysis%2C%20the%20elbow,number%20of%20clusters%20to%20use.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;elbow method&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In short, the elbow method plots a curve of the number of clusters vs percentage of explained variation. The curve produced by the elbow method is used by practitioners to determine the optimal number of clusters by following the &lt;a href=&#34;https://en.wikipedia.org/wiki/Diminishing_returns&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;law of diminishing returns&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If adding an extra cluster does not significantly improve the variation of k, we choose to stick to the current number of clusters.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;tips-and-optimizations&#34;&gt;Tips and Optimizations&lt;/h3&gt;
&lt;p&gt;Here are a couple of tips to ensure good clustering is obtained:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Removing non-numeric features&lt;/strong&gt;:¬†&lt;br&gt;
Data may have non-numeric (categorical) features represented as numeric features. Instead of the numbers having some quantitative value, they might be used as labels for a group. For eg. if we are dealing with a population dataset, a column named ‚ÄúGender‚Äù may have values 0 and 1 representing Male and Female. We must be careful in removing these features as they do not have any quantitative value and hence, will distort our algorithm‚Äôs notion of ‚Äòdistance‚Äô.&lt;/li&gt;
&lt;li&gt;**Feature Scaling:&lt;br&gt;
**Numeric data will have different ranges. A particular feature with a huge range may adversely impact our clustering objective function. The feature with the big range values will dominate the clustering process over other features. Hence, it is crucial to scale our data so that the contribution of each feature is proportional to the algorithm.&lt;/li&gt;
&lt;li&gt;**Better Initialization:&lt;br&gt;
**In our algorithm, we randomly assign the initial clusters to the data. Because of this inherent randomness, our algorithm may not always provide good clusters. There are a couple of ways by which the criterion for setting the initial clusters is improved. The &lt;a href=&#34;https://en.wikipedia.org/wiki/K-means%2B%2B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;k-means++&lt;/a&gt; algorithm is a popular choice for this task.&lt;/li&gt;
&lt;li&gt;**Different Algorithms:&lt;br&gt;
**There are certain algorithms that are variants of the k-means algorithm which are more robust in handling certain constraints such as outliers. One such algorithm is the &lt;a href=&#34;https://en.wikipedia.org/wiki/K-medoids&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;k-medoids&lt;/a&gt;. The k-medoids algorithm uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Taxicab_geometry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;L1 distance&lt;/a&gt; instead of L2 distance (Euclidean distance). There are a bunch of other clustering algorithms that are useful for specific applications such as hierarchal clustering, density-based clustering, fuzzy clustering, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I hope all of you enjoyed this blog post. For more articles on Data Science check out my other posts on medium. Feel free to connect with me on &lt;a href=&#34;https://www.linkedin.com/in/sayarbanerjee/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LinkedIn&lt;/a&gt;. The code for this blog post is on my &lt;a href=&#34;https://github.com/Sayar1106/TowardsDataSciencecodefiles/blob/master/Kmeansfromscratch/kmeans.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://scikit-learn.org/stable/datasets/index.html#datasets&#34; title=&#34;https://scikit-learn.org/stable/datasets/index.html#datasets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;scikit-learn 0.23.1 documentation&lt;/strong&gt;&lt;/a&gt;&lt;a href=&#34;https://scikit-learn.org/stable/datasets/index.html#datasets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/K-means_clustering&#34; title=&#34;https://en.wikipedia.org/wiki/K-means_clustering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;k-means clustering&lt;/strong&gt;&lt;/a&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/K-means_clustering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/data-science-from/9781492041122/&#34; title=&#34;https://www.oreilly.com/library/view/data-science-from/9781492041122/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Data Science from Scratch, 2nd Edition&lt;/strong&gt;&lt;/a&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/data-science-from/9781492041122/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://Sayar1106.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://Sayar1106.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://Sayar1106.github.io/publication/example/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://Sayar1106.github.io/publication/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://Sayar1106.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://Sayar1106.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
